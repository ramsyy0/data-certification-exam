{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spotify Popularity Predictor (39%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this challenge is to create a model that predicts the popularity of a song based on its features.\n",
    "\n",
    "The dataset contains a list of tracks with the following characteristics:\n",
    "- `acousticness`: whether the track is acoustic\n",
    "- `danceability`: describes how suitable a track is for dancing\n",
    "- `duration_ms`: duration of the track in milliseconds\n",
    "- `energy`: represents a perceptual measure of intensity and activity\n",
    "- `explicit`: whether the track has explicit lyrics\n",
    "- `id`: id for the track\n",
    "- `instrumentalness`: predicts whether a track contains no vocals\n",
    "- `key`: the key the track is in\n",
    "- `liveness`: detects the presence of an audience in the recording\n",
    "- `loudness`: the overall loudness of a track in decibels\n",
    "- `mode`: modality of a track\n",
    "- `name`: name of the track\n",
    "- `popularity`: popularity of the track\n",
    "- `release_date`: release date\n",
    "- `speechiness`: detects the presence of spoken words in a track\n",
    "- `tempo`: overall estimated tempo of a track in beats per minute\n",
    "- `valence`: describes the musical positiveness conveyed by a track\n",
    "- `artist`: artist who performed the track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "\n",
    "**üìù Load the `spotify_popularity_train.csv` dataset from the provided URL. Display the first few rows. Perform the usual cleaning operations. Store the result in a `DataFrame` named `data`.**\n",
    "\n",
    "üëâ Do not forget to clean the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://wagon-public-datasets.s3.amazonaws.com/certification_paris_2021Q1/spotify_popularity_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(url)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your results\n",
    "\n",
    "Run the following cell to save your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "ChallengeResult(\n",
    "    \"c5_data_cleaning\",\n",
    "    data=data).write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üìù We want to use a metric that measures the prediction error in the same unit than `popularity`. In addition, it should strongly penalize largest errors. Which sklearn's [metric](https://scikit-learn.org/stable/modules/model_evaluation.html) should we use? Store its exact name as string below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = \"?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üìù Let's build a baseline model using only the numerical features in our dataset.**\n",
    "- Build `X_baseline` with only numerical features\n",
    "- Build `y` your target containing the `popularity`\n",
    "- Then 5 times cross validate the baseline linear model of your choice (do not fine tune it)\n",
    "- Store your mean performance in a `float` variable named `baseline_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your results\n",
    "\n",
    "Run the following cell to save your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "ChallengeResult(\n",
    "    \"baseline\",\n",
    "    scoring=scoring,\n",
    "    baseline_score=baseline_score).write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Let's now use the features that we left aside: `release_date` and `artist` to improve the performance of our model. We'll create them manually in a train vs. test context first (and pipeline them later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### holdout\n",
    "**üìù Create the 4 variables `X_train` `y_train`, `X_test`, `y_test` with a 50% split with random sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### year\n",
    "\n",
    "**üìù Create `X_train_year` and `X_test_year` by adding the new column `year` containing the release year of the track as integer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### artist\n",
    "\n",
    "How could we use the `artist` column? There are too many artists to one hot encode it.  \n",
    "We could instead create an `artist_popularity` feature containing the mean popularity of an artist, computed as the mean popularity of all tracks the artist released _on the train set_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process artist popularity from the Training set\n",
    "\n",
    "**üìù Compute and store the `artist_popularity` as a new pandas `Series`**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the artist popularity to `X_train_year`\n",
    "\n",
    "**üìù Create a new DataFrame `X_train_engineered` which adds a new column to the existing `X_train_year` with the `artist_popularity` corresponding to the song's artist.** \n",
    "\n",
    "üö® Make sure that the target `popularity` does not end up in `X_train_engineered` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the artist popularity to `X_test_year`\n",
    "\n",
    "**üìù Similarily, create a new DataFrame `X_test_engineered` which also adds a new column to the existing `X_test_year` with the `artist_popularity` corresponding to the song's artist, computed from the training set.**\n",
    "\n",
    "üö®**If an artist has never been seen in the training set, use the global mean popularity of all the tracks of `X_train`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your results\n",
    "\n",
    "Run the following cell to save your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "_ = pd.concat([X_train_engineered, X_test_engineered])\n",
    "\n",
    "ChallengeResult(\"c7_feature_engineering\",\n",
    "    shape = _.shape,\n",
    "    cols = _.columns,\n",
    "    years = _.get(\"year\"),\n",
    "    popularities = _.get(\"artist_popularity\"),\n",
    ").write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance\n",
    "\n",
    "**üìù Let's see how these features impact the performance of our model. Retrain the same baseline model on numerical values only, but adding the new features `year` and `artist_popularity`, and see how the performance is impacted. Save the performance in a `float` variable named `score_engineered`**\n",
    "\n",
    "üëâ Do not fine tune the model yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your results\n",
    "\n",
    "Run the following cell to save your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "ChallengeResult(\n",
    "    \"c7_score_engineering\",\n",
    "    scoring=scoring,\n",
    "    score_engineered=score_engineered).write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelining\n",
    "\n",
    "**üìù Let's create a full sklearn preprocessing pipeline called `preproc`. It should integrate our feature engineering for `year` and `artist_popularity`, as well as any other preprocessing of your choice**\n",
    "\n",
    "**Store also the number of columns/feature after preprocessing your inputs in a variable `col_number`**\n",
    "\n",
    "**üö®‚ö†Ô∏è Advice: SKIP the `ArtistPopularityTransformer` if you don't have time to do it. It is better for you to have a working pipeline rather than NO pipeline at all**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# üëâ Do not hesitate to reload clean new dataset if you need a fresh start\n",
    "y = data.popularity\n",
    "X = data.drop(\"popularity\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to visualize your pipeline as you build it\n",
    "from sklearn import set_config; set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We give you below the skeleton of the custom ArtistPopularityTransformer to complete\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ArtistPopularityTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        process artist mean popularity from artists songs popularity\n",
    "        process song global mean popularity\n",
    "        \"\"\"\n",
    "\n",
    "        # process artist popularity\n",
    "\n",
    "        # process mean popularity\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        apply artist mean popularity vs song global mean popularity to songs\n",
    "        \"\"\"\n",
    "\n",
    "        # inject artist popularity\n",
    "\n",
    "        # fills popularity of unknown artists with song global mean popularity\n",
    "\n",
    "        return # TODO return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save your results\n",
    "\n",
    "Run the following cell to save your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print below your preproc here for the correctors\n",
    "from sklearn import set_config; set_config(display='diagram')\n",
    "preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "ChallengeResult(\n",
    "    \"c6_preprocessing\",\n",
    "    col_number=col_number\n",
    ").write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "üìù Time to optimize \n",
    "\n",
    "- **Add an estimator to your pipeline (only from scikit-learn)** \n",
    "\n",
    "- **Train your pipeline and fine-tune (optimize) your estimator to get the best prediction score**\n",
    "\n",
    "- **You must create 2 pipelines (one with a linear model, one with an ensemble model)**\n",
    "\n",
    "Then, \n",
    "\n",
    "- Save your two best 5-time cross-validated scores as _float_: `score_linear` and `score_ensemble`\n",
    "\n",
    "- Save your two best trained pipelines as _Pipeline_ objects: `pipe_linear` and `pipe_ensemble`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your results\n",
    "\n",
    "Run the following cell to save your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print below your best pipe for correction purpose\n",
    "from sklearn import set_config; set_config(display='diagram')\n",
    "pipe_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print below your best pipe for correction purpose\n",
    "pipe_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "ChallengeResult(\"c8_c9_c11_c13_model_tuning\",\n",
    "    scoring = scoring,\n",
    "    score_linear=score_linear,\n",
    "    score_ensemble=score_ensemble).write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API \n",
    "\n",
    "Time to put a pipeline in production!\n",
    "\n",
    "üëâ Go to https://github.com/lewagon/data-certification-api and follow instructions\n",
    "\n",
    "**This final part is independent from the above notebook**"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "252.003px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
